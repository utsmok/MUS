{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This sets up the Django environment '''\n",
    "import os\n",
    "import django\n",
    "PROJECTPATH = \"\"\n",
    "os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"mus.settings\")\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"  # https://docs.djangoproject.com/en/4.1/topics/async/#async-safety\n",
    "django.setup()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi', 'isbn', 'researchutwente', 'risutwente', 'scopus'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PureOpenAlex.models import Paper, PureEntry\n",
    "from django.db.models import Q\n",
    "from django.db import transaction\n",
    "from pprint import pprint\n",
    "keys=set()\n",
    "for item in PureEntry.objects.exclude(duplicate_ids=dict()).only('id','duplicate_ids'):\n",
    "    for key in item.duplicate_ids.keys():\n",
    "        keys.add(key)\n",
    "\n",
    "keys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Paper: Paper object (45790)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PureEntry.objects.get(pk=51688).paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# pure authors in db 84733\n",
      "# pure authors in set (unique names) 83350\n",
      "# authors in db 71001\n",
      "# authors in set (unique names) 70875\n"
     ]
    }
   ],
   "source": [
    "from PureOpenAlex.models import PureAuthor, Author\n",
    "from django.db import transaction\n",
    "from nameparser import HumanName\n",
    "from PureOpenAlex.namematcher import NameMatcher\n",
    "from unidecode import unidecode\n",
    "from pprint import pprint\n",
    "\n",
    "allpureauthors = PureAuthor.objects.all()\n",
    "purenames={}\n",
    "purefullnames = {}\n",
    "pureinitials = {}\n",
    "for author in allpureauthors:\n",
    "    hname=HumanName(unidecode(author.name),initials_format=\"{first} {middle}\")\n",
    "    purenames[author.id] = {\n",
    "        'full': hname.full_name,\n",
    "        'initials': hname.initials()+\" \"+hname.last\n",
    "    }\n",
    "    purefullnames[hname.full_name]=author.id\n",
    "    pureinitials[hname.initials()+\" \"+hname.last]=author.id\n",
    "allauthors = Author.objects.all()\n",
    "\n",
    "authnames={}\n",
    "authfullnames = {}\n",
    "authinitials = {}\n",
    "for author in allauthors:\n",
    "    hname=HumanName(unidecode(author.name),initials_format=\"{first} {middle}\")\n",
    "    authnames[author.id] = {\n",
    "        'full': hname.full_name,\n",
    "        'initials': hname.initials()+\" \"+hname.last\n",
    "    }\n",
    "    authfullnames[hname.full_name]=author.id\n",
    "    authinitials[hname.initials()+\" \"+hname.last]=author.id\n",
    "\n",
    "\n",
    "purefullnameset=set(purefullnames.keys())\n",
    "pureinitialsset=set(pureinitials.keys())\n",
    "\n",
    "authfullnameset=set(authfullnames.keys())\n",
    "authinitialsset=set(authinitials.keys())\n",
    "\n",
    "print('# pure authors in db',allpureauthors.count())\n",
    "print('# pure authors in set (unique names)',len(purefullnameset))\n",
    "print('# authors in db',allauthors.count())\n",
    "print('# authors in set (unique names)',len(authfullnameset))\n",
    "\n",
    "intersection = purefullnameset.intersection(authfullnameset)\n",
    "print('# common names',len(intersection))\n",
    "\n",
    "listtosave=[]\n",
    "from PureOpenAlex.models import PureEntry\n",
    "\n",
    "j=0\n",
    "h=0\n",
    "\n",
    "for i,name in enumerate(intersection):\n",
    "    pureauthorid=purefullnames[name]\n",
    "    authorid=authfullnames[name]\n",
    "    pureauthor = PureAuthor.objects.get(id=pureauthorid)\n",
    "    pureentries = pureauthor.pure_entries.all()\n",
    "    pureentry_c = pureauthor.pure_creators.all()\n",
    "    author=Author.objects.get(id=authorid)\n",
    "    for entry in pureentries:\n",
    "        if author not in entry.authors.all():\n",
    "            entry.authors.add(author)\n",
    "            listtosave.append(entry)\n",
    "            h=h+1\n",
    "    for entry in pureentry_c:\n",
    "        if author not in entry.authors.all():\n",
    "            entry.authors.add(author)\n",
    "            listtosave.append(entry)\n",
    "            j=j+1\n",
    "    if i%1000==0:\n",
    "        print('# of entries that need updating:', len(listtosave))\n",
    "        print('# of intersections checked:', i)\n",
    "        print('pureentries:',h)\n",
    "        print('purecreators:',j)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5972\n",
      "5851\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from PureOpenAlex.models import Author,PureEntry\n",
    "from django.db.models import Q\n",
    "noauths=PureEntry.objects.filter(authors__isnull=True).distinct()\n",
    "print(noauths.count())\n",
    "print(noauths.filter(Q(creators__isnull=False) | Q(contributors__isnull=False)).distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 UTData entries with 104 related Departments processed\n",
      "200 UTData entries with 208 related Departments processed\n",
      "300 UTData entries with 312 related Departments processed\n",
      "400 UTData entries with 413 related Departments processed\n",
      "500 UTData entries with 519 related Departments processed\n",
      "600 UTData entries with 627 related Departments processed\n",
      "700 UTData entries with 728 related Departments processed\n",
      "800 UTData entries with 832 related Departments processed\n",
      "900 UTData entries with 946 related Departments processed\n",
      "1000 UTData entries with 1054 related Departments processed\n",
      "1100 UTData entries with 1162 related Departments processed\n",
      "1200 UTData entries with 1263 related Departments processed\n",
      "1300 UTData entries with 1364 related Departments processed\n",
      "1400 UTData entries with 1467 related Departments processed\n",
      "1500 UTData entries with 1574 related Departments processed\n",
      "1600 UTData entries with 1680 related Departments processed\n",
      "1700 UTData entries with 1783 related Departments processed\n",
      "1800 UTData entries with 1885 related Departments processed\n",
      "1900 UTData entries with 1992 related Departments processed\n",
      "2000 UTData entries with 2095 related Departments processed\n",
      "2100 UTData entries with 2197 related Departments processed\n",
      "2200 UTData entries with 2300 related Departments processed\n",
      "2300 UTData entries with 2402 related Departments processed\n",
      "2400 UTData entries with 2508 related Departments processed\n",
      "2500 UTData entries with 2612 related Departments processed\n",
      "2600 UTData entries with 2716 related Departments processed\n",
      "2700 UTData entries with 2820 related Departments processed\n",
      "2800 UTData entries with 2922 related Departments processed\n",
      "2900 UTData entries with 3025 related Departments processed\n",
      "3000 UTData entries with 3131 related Departments processed\n",
      "3100 UTData entries with 3233 related Departments processed\n",
      "3200 UTData entries with 3335 related Departments processed\n",
      "3300 UTData entries with 3439 related Departments processed\n",
      "3309 UTData entries with 3448 related Departments processed\n"
     ]
    }
   ],
   "source": [
    "def migrate_department_data():\n",
    "    from PureOpenAlex.models import UTData, Department\n",
    "    from collections import defaultdict\n",
    "    from django.db import transaction\n",
    "\n",
    "    utdatalist=UTData.objects.all().only(\"departments\").prefetch_related(\"departments\")\n",
    "    facultylist = ['EEMCS', 'BMS', 'ET', 'ITC', 'TNW']\n",
    "    savelist=[]\n",
    "    i=0\n",
    "    j=0\n",
    "    for data in utdatalist:\n",
    "        depts=list(data.departments.all())\n",
    "        i=i+1\n",
    "        data.employment_data = defaultdict(list)\n",
    "        if not depts:\n",
    "            data.current_faculty=\"\"\n",
    "            data.current_group=\"\"\n",
    "            data.employment_data['employment'].append({})\n",
    "        elif len(depts)==1:\n",
    "            j=j+1\n",
    "            data.current_faculty=depts[0].faculty\n",
    "            data.current_group=depts[0].name\n",
    "            data.employment_data['employment'].append({'faculty':depts[0].faculty,'group':depts[0].name})\n",
    "        else:\n",
    "            current=False\n",
    "            for dept in depts:\n",
    "                j=j+1\n",
    "                if not current:\n",
    "                    if dept.faculty in facultylist:\n",
    "                        data.current_faculty=dept.faculty\n",
    "                        data.current_group=dept.name\n",
    "                        current=True\n",
    "                data.employment_data['employment'].append({'faculty':dept.faculty,'group':dept.name})\n",
    "            if not current:\n",
    "                data.current_faculty=data.employment_data['employment'][0]['faculty']\n",
    "                data.current_group=data.employment_data['employment'][0]['group']\n",
    "        savelist.append(data)\n",
    "        if i%100==0 or i==len(utdatalist):\n",
    "            print(f'{i} UTData entries with {j} related Departments processed')\n",
    "            with transaction.atomic():\n",
    "                UTData.objects.bulk_update(savelist, ['current_faculty', 'current_group', 'employment_data'])\n",
    "            savelist=[]\n",
    "\n",
    "\n",
    "migrate_department_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ 11 total: 11\n",
      "+ 13 total: 24\n",
      "+ 40 total: 64\n",
      "+ 28 total: 92\n",
      "+ 34 total: 126\n",
      "+ 36 total: 162\n",
      "+ 26 total: 188\n",
      "+ 21 total: 209\n",
      "+ 19 total: 228\n",
      "+ 10 total: 238\n",
      "+ 16 total: 254\n",
      "+ 14 total: 268\n",
      "+ 7 total: 275\n",
      "+ 7 total: 282\n",
      "+ 7 total: 289\n",
      "+ 11 total: 300\n",
      "+ 3 total: 303\n",
      "+ 4 total: 307\n",
      "+ 2 total: 309\n",
      "+ 4 total: 313\n",
      "+ 5 total: 318\n",
      "+ 5 total: 323\n",
      "+ 8 total: 331\n",
      "+ 29 total: 360\n",
      "+ 26 total: 386\n",
      "+ 14 total: 400\n",
      "+ 27 total: 427\n",
      "+ 15 total: 442\n",
      "+ 16 total: 458\n",
      "+ 14 total: 472\n",
      "+ 30 total: 502\n",
      "+ 39 total: 541\n"
     ]
    }
   ],
   "source": [
    "from django.db import transaction\n",
    "from PureOpenAlex.models import (\n",
    "    PureEntry,\n",
    "    Paper,\n",
    ")\n",
    "\n",
    "def matchPureEntryWithPaper():\n",
    "    \"\"\"\n",
    "    For every PureEntry, try to find a matching paper in the database and mark them as such.\n",
    "    \"\"\"\n",
    "    paperlist = []\n",
    "    entrylist = []\n",
    "    i=0\n",
    "    j=0\n",
    "    allentries = PureEntry.objects.all().only(\"doi\",\"title\",'researchutwente', 'risutwente', 'other_links', 'duplicate_ids')\n",
    "    paperpreload = Paper.objects.all().only(\"doi\",\"title\",'locations','id').prefetch_related('locations')\n",
    "    for entry in allentries:\n",
    "        j=j+1\n",
    "        found=False\n",
    "        if entry.paper != None or entry.paper == \"\":\n",
    "            found=True\n",
    "        paper = None\n",
    "        if entry.doi != \"\":\n",
    "            paper = paperpreload.filter(doi=entry.doi).first()\n",
    "        if not paper and entry.risutwente != \"\":\n",
    "            paper = paperpreload.filter(locations__pdf_url__icontains=entry.risutwente).first()\n",
    "        if not paper and entry.researchutwente != \"\":\n",
    "            paper = paperpreload.filter(locations__pdf_url__icontains=entry.researchutwente).first()\n",
    "        if not paper and entry.risutwente != \"\":\n",
    "            paper = paperpreload.filter(locations__landing_page_url__icontains=entry.risutwente).first()\n",
    "        if not paper and entry.researchutwente != \"\":\n",
    "            paper = paperpreload.filter(locations__landing_page_url__icontains=entry.researchutwente).first()\n",
    "        if not paper:\n",
    "            paper = paperpreload.filter(title__icontains=entry.title).first()\n",
    "        if not paper and entry.duplicate_ids!={}:\n",
    "            for key, value in entry.duplicate_ids.items():\n",
    "                if not paper:\n",
    "                    if key == 'doi':\n",
    "                        paper = paperpreload.filter(doi=value).first()\n",
    "                    if key == 'risutwente' or key == 'researchutwente':\n",
    "                        paper = paperpreload.filter(locations__pdf_url__icontains=value).first()\n",
    "                        paper = paperpreload.filter(locations__landing_page_url__icontains=value).first()\n",
    "        if not paper and entry.other_links!={}:\n",
    "            if 'other' in entry.other_links:\n",
    "                for value in entry.other_links['other']:\n",
    "                    if not paper:\n",
    "                        paper = paperpreload.filter(locations__pdf_url__icontains=value).first()\n",
    "                        paper = paperpreload.filter(locations__landing_page_url__icontains=value).first()\n",
    "\n",
    "        if paper and not found:\n",
    "            entry.paper = paper\n",
    "            paper.has_pure_oai_match = True\n",
    "            paperlist.append(paper)\n",
    "            entrylist.append(entry)\n",
    "        elif paper and found:\n",
    "            if paper.title != entry.paper.title or paper.doi != entry.paper.doi:\n",
    "                entry.paper = paper\n",
    "                paper.has_pure_oai_match = True\n",
    "                paperlist.append(paper)\n",
    "                entrylist.append(entry)\n",
    "        else:\n",
    "            pass #no match or no new match\n",
    "        if j == 1000:\n",
    "            with transaction.atomic():\n",
    "                Paper.objects.bulk_update(paperlist, fields=[\"has_pure_oai_match\"])\n",
    "                PureEntry.objects.bulk_update(entrylist, fields=[\"paper\"])\n",
    "            i=i+len(paperlist)\n",
    "            print(\"+\",str(len(paperlist)), \"total:\", str(i))\n",
    "            j=0\n",
    "            paperlist = []\n",
    "            entrylist = []\n",
    "\n",
    "\n",
    "    return True\n",
    "\n",
    "matchPureEntryWithPaper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 entries done\n",
      "3472 identifiers processed in total\n",
      "2000 entries done\n",
      "6964 identifiers processed in total\n",
      "3000 entries done\n",
      "10444 identifiers processed in total\n",
      "4000 entries done\n",
      "13984 identifiers processed in total\n",
      "5000 entries done\n",
      "17474 identifiers processed in total\n",
      "6000 entries done\n",
      "20834 identifiers processed in total\n",
      "7000 entries done\n",
      "24367 identifiers processed in total\n",
      "8000 entries done\n",
      "28158 identifiers processed in total\n",
      "9000 entries done\n",
      "31636 identifiers processed in total\n",
      "10000 entries done\n",
      "35075 identifiers processed in total\n",
      "11000 entries done\n",
      "38420 identifiers processed in total\n",
      "12000 entries done\n",
      "41778 identifiers processed in total\n",
      "13000 entries done\n",
      "45344 identifiers processed in total\n",
      "14000 entries done\n",
      "49031 identifiers processed in total\n",
      "15000 entries done\n",
      "52698 identifiers processed in total\n",
      "16000 entries done\n",
      "56292 identifiers processed in total\n",
      "17000 entries done\n",
      "59767 identifiers processed in total\n",
      "18000 entries done\n",
      "63482 identifiers processed in total\n",
      "19000 entries done\n",
      "67202 identifiers processed in total\n",
      "20000 entries done\n",
      "70980 identifiers processed in total\n",
      "21000 entries done\n",
      "74630 identifiers processed in total\n",
      "22000 entries done\n",
      "78327 identifiers processed in total\n",
      "23000 entries done\n",
      "81896 identifiers processed in total\n",
      "24000 entries done\n",
      "85559 identifiers processed in total\n",
      "25000 entries done\n",
      "89106 identifiers processed in total\n",
      "26000 entries done\n",
      "92872 identifiers processed in total\n",
      "27000 entries done\n",
      "96568 identifiers processed in total\n",
      "28000 entries done\n",
      "100259 identifiers processed in total\n",
      "29000 entries done\n",
      "103932 identifiers processed in total\n",
      "30000 entries done\n",
      "107566 identifiers processed in total\n",
      "31000 entries done\n",
      "110988 identifiers processed in total\n",
      "32000 entries done\n",
      "113917 identifiers processed in total\n",
      "33000 entries done\n",
      "117119 identifiers processed in total\n",
      "34000 entries done\n",
      "120312 identifiers processed in total\n",
      "35000 entries done\n",
      "124077 identifiers processed in total\n",
      "36000 entries done\n",
      "127911 identifiers processed in total\n",
      "37000 entries done\n",
      "131942 identifiers processed in total\n",
      "38000 entries done\n",
      "135942 identifiers processed in total\n",
      "39000 entries done\n",
      "139960 identifiers processed in total\n",
      "40000 entries done\n",
      "143852 identifiers processed in total\n",
      "41000 entries done\n",
      "147609 identifiers processed in total\n",
      "42000 entries done\n",
      "151348 identifiers processed in total\n",
      "43000 entries done\n",
      "155100 identifiers processed in total\n",
      "44000 entries done\n",
      "159122 identifiers processed in total\n",
      "45000 entries done\n",
      "162834 identifiers processed in total\n",
      "46000 entries done\n",
      "166537 identifiers processed in total\n",
      "47000 entries done\n",
      "170395 identifiers processed in total\n",
      "48000 entries done\n",
      "174269 identifiers processed in total\n",
      "49000 entries done\n",
      "178271 identifiers processed in total\n",
      "50000 entries done\n",
      "182157 identifiers processed in total\n",
      "51000 entries done\n",
      "186052 identifiers processed in total\n",
      "52000 entries done\n",
      "190042 identifiers processed in total\n",
      "53000 entries done\n",
      "193983 identifiers processed in total\n",
      "54000 entries done\n",
      "197870 identifiers processed in total\n",
      "55000 entries done\n",
      "201817 identifiers processed in total\n",
      "56000 entries done\n",
      "205738 identifiers processed in total\n",
      "57000 entries done\n",
      "209412 identifiers processed in total\n",
      "58000 entries done\n",
      "213240 identifiers processed in total\n",
      "59000 entries done\n",
      "216862 identifiers processed in total\n",
      "60000 entries done\n",
      "220314 identifiers processed in total\n",
      "61000 entries done\n",
      "224128 identifiers processed in total\n",
      "62000 entries done\n",
      "228022 identifiers processed in total\n",
      "63000 entries done\n",
      "231674 identifiers processed in total\n",
      "64000 entries done\n",
      "235390 identifiers processed in total\n",
      "65000 entries done\n",
      "239182 identifiers processed in total\n",
      "66000 entries done\n",
      "243013 identifiers processed in total\n",
      "67000 entries done\n",
      "246507 identifiers processed in total\n",
      "68000 entries done\n",
      "250395 identifiers processed in total\n",
      "69000 entries done\n",
      "254314 identifiers processed in total\n",
      "70000 entries done\n",
      "258374 identifiers processed in total\n",
      "71000 entries done\n",
      "262307 identifiers processed in total\n",
      "72000 entries done\n",
      "266360 identifiers processed in total\n",
      "73000 entries done\n",
      "270408 identifiers processed in total\n",
      "74000 entries done\n",
      "274398 identifiers processed in total\n",
      "75000 entries done\n",
      "278286 identifiers processed in total\n",
      "76000 entries done\n",
      "282206 identifiers processed in total\n",
      "77000 entries done\n",
      "286126 identifiers processed in total\n",
      "78000 entries done\n",
      "290188 identifiers processed in total\n",
      "79000 entries done\n",
      "294244 identifiers processed in total\n",
      "80000 entries done\n",
      "298240 identifiers processed in total\n",
      "81000 entries done\n",
      "302178 identifiers processed in total\n",
      "82000 entries done\n",
      "306217 identifiers processed in total\n",
      "83000 entries done\n",
      "310304 identifiers processed in total\n",
      "84000 entries done\n",
      "314282 identifiers processed in total\n",
      "85000 entries done\n",
      "318384 identifiers processed in total\n",
      "86000 entries done\n",
      "322335 identifiers processed in total\n",
      "87000 entries done\n",
      "326357 identifiers processed in total\n",
      "88000 entries done\n",
      "330424 identifiers processed in total\n",
      "89000 entries done\n",
      "334370 identifiers processed in total\n",
      "90000 entries done\n",
      "337666 identifiers processed in total\n",
      "91000 entries done\n",
      "340972 identifiers processed in total\n",
      "92000 entries done\n",
      "344210 identifiers processed in total\n",
      "93000 entries done\n",
      "347360 identifiers processed in total\n",
      "94000 entries done\n",
      "350652 identifiers processed in total\n",
      "95000 entries done\n",
      "353868 identifiers processed in total\n",
      "96000 entries done\n",
      "357244 identifiers processed in total\n",
      "97000 entries done\n",
      "360661 identifiers processed in total\n",
      "98000 entries done\n",
      "364424 identifiers processed in total\n",
      "99000 entries done\n",
      "368376 identifiers processed in total\n",
      "100000 entries done\n",
      "372408 identifiers processed in total\n",
      "101000 entries done\n",
      "375672 identifiers processed in total\n",
      "102000 entries done\n",
      "379319 identifiers processed in total\n",
      "103000 entries done\n",
      "383240 identifiers processed in total\n",
      "104000 entries done\n",
      "386684 identifiers processed in total\n",
      "105000 entries done\n",
      "389758 identifiers processed in total\n",
      "106000 entries done\n",
      "392822 identifiers processed in total\n",
      "107000 entries done\n",
      "395806 identifiers processed in total\n",
      "108000 entries done\n",
      "398712 identifiers processed in total\n",
      "109000 entries done\n",
      "402026 identifiers processed in total\n",
      "110000 entries done\n",
      "405579 identifiers processed in total\n",
      "111000 entries done\n",
      "409037 identifiers processed in total\n",
      "112000 entries done\n",
      "412274 identifiers processed in total\n",
      "113000 entries done\n",
      "415478 identifiers processed in total\n",
      "114000 entries done\n",
      "418883 identifiers processed in total\n",
      "115000 entries done\n",
      "422178 identifiers processed in total\n",
      "116000 entries done\n",
      "426092 identifiers processed in total\n",
      "117000 entries done\n",
      "430054 identifiers processed in total\n",
      "118000 entries done\n",
      "433965 identifiers processed in total\n",
      "119000 entries done\n",
      "437834 identifiers processed in total\n",
      "120000 entries done\n",
      "441868 identifiers processed in total\n",
      "121000 entries done\n",
      "445298 identifiers processed in total\n",
      "122000 entries done\n",
      "448991 identifiers processed in total\n",
      "123000 entries done\n",
      "452664 identifiers processed in total\n",
      "124000 entries done\n",
      "456546 identifiers processed in total\n",
      "125000 entries done\n",
      "460290 identifiers processed in total\n",
      "126000 entries done\n",
      "464236 identifiers processed in total\n",
      "127000 entries done\n",
      "468198 identifiers processed in total\n",
      "128000 entries done\n",
      "472226 identifiers processed in total\n",
      "129000 entries done\n",
      "476217 identifiers processed in total\n",
      "130000 entries done\n",
      "480247 identifiers processed in total\n",
      "131000 entries done\n",
      "484182 identifiers processed in total\n",
      "132000 entries done\n",
      "488168 identifiers processed in total\n",
      "133000 entries done\n",
      "492181 identifiers processed in total\n",
      "134000 entries done\n",
      "496331 identifiers processed in total\n",
      "135000 entries done\n",
      "500856 identifiers processed in total\n",
      "136000 entries done\n",
      "504600 identifiers processed in total\n",
      "137000 entries done\n",
      "507992 identifiers processed in total\n",
      "138000 entries done\n",
      "511955 identifiers processed in total\n",
      "139000 entries done\n",
      "516004 identifiers processed in total\n",
      "140000 entries done\n",
      "520157 identifiers processed in total\n",
      "141000 entries done\n",
      "524434 identifiers processed in total\n",
      "142000 entries done\n",
      "528924 identifiers processed in total\n",
      "143000 entries done\n",
      "533298 identifiers processed in total\n",
      "144000 entries done\n",
      "536960 identifiers processed in total\n",
      "145000 entries done\n",
      "540257 identifiers processed in total\n",
      "146000 entries done\n",
      "543652 identifiers processed in total\n",
      "147000 entries done\n",
      "546910 identifiers processed in total\n",
      "148000 entries done\n",
      "550530 identifiers processed in total\n",
      "149000 entries done\n",
      "553938 identifiers processed in total\n"
     ]
    }
   ],
   "source": [
    "from PureOpenAlex.models import Identifier, PureEntry\n",
    "from collections import defaultdict\n",
    "from django.db import transaction\n",
    "\n",
    "MATCHURLCONTENT = {\n",
    "    \"itc.utwente.nl\": \"itc_content\",\n",
    "    \"www.itc.nl\": \"itc_content\",\n",
    "    \"arxiv\": \"arxiv\",\n",
    "    \"zenodo\": \"zenodo\",\n",
    "    \"github\": \"github\",\n",
    "    \"https://10.\": \"doi\",\n",
    "    \"http://10.\": \"doi\",\n",
    "}\n",
    "MATCHIDTYPES = {\n",
    "\"doi\": '',\n",
    "\"isbn\": '',\n",
    "\"researchutwente\": '',\n",
    "\"risutwente\": '',\n",
    "\"scopus\": ''\n",
    "}\n",
    "bulklist=[]\n",
    "i=0\n",
    "j=0\n",
    "allentries=PureEntry.objects.all().filter(identifiers__isnull=False).only('doi', 'isbn', 'researchutwente', 'risutwente', 'scopus', 'other_links','id', 'duplicate_ids').prefetch_related(\"identifiers\")\n",
    "for entry in allentries:\n",
    "    entry.doi = \"\"\n",
    "    entry.isbn = \"\"\n",
    "    entry.researchutwente = \"\"\n",
    "    entry.risutwente = \"\"\n",
    "    entry.scopus = \"\"\n",
    "    entry.other_links= defaultdict(list)\n",
    "    entry.duplicate_ids = defaultdict(list)\n",
    "    for identifier in entry.identifiers.all():\n",
    "        j=j+1\n",
    "        duplicate=False\n",
    "        if 'https://ezproxy2.utwente.nl/login?url=' in identifier.url:\n",
    "            identifier.url = identifier.url.replace('https://ezproxy2.utwente.nl/login?url=','')\n",
    "        if str(identifier.idtype) in MATCHIDTYPES.keys():\n",
    "            if str(identifier.idtype) == 'doi':\n",
    "                identifier.url = identifier.url.replace('doi.org1','doi.org/1')\n",
    "                if entry.doi == \"\" or entry.doi == None:\n",
    "                    entry.doi = identifier.url\n",
    "                else:\n",
    "                    duplicate=True\n",
    "            if str(identifier.idtype) == 'isbn':\n",
    "                identifier.url = identifier.url.strip('urn:ISBN:')\n",
    "                if entry.isbn == \"\" or entry.isbn == None:\n",
    "                    entry.isbn = identifier.url\n",
    "                else:\n",
    "                    duplicate=True\n",
    "            if identifier.idtype == 'researchutwente':\n",
    "                if entry.researchutwente == \"\" or entry.researchutwente == None:\n",
    "                    entry.researchutwente = identifier.url\n",
    "                else:\n",
    "                    duplicate=True\n",
    "            if identifier.idtype == 'risutwente':\n",
    "                if entry.risutwente == \"\" or entry.risutwente == None:\n",
    "                    entry.risutwente = identifier.url\n",
    "                else:\n",
    "                    duplicate=True\n",
    "            if identifier.idtype == 'scopus':\n",
    "                if entry.scopus == \"\" or entry.scopus == None:\n",
    "                    entry.scopus = identifier.url\n",
    "                else:\n",
    "                    duplicate=True\n",
    "            if duplicate:\n",
    "                entry.duplicate_ids[str(identifier.idtype)].append(identifier.url)\n",
    "        else:\n",
    "            matched=False\n",
    "            for key, value in MATCHURLCONTENT.items():\n",
    "                if key in identifier.url and not matched:\n",
    "                    if value != \"doi\":\n",
    "                        entry.other_links[value].append(identifier.url)\n",
    "                        matched=True\n",
    "                    else: # doi with wrong formatting found\n",
    "                        identifier.url = identifier.url.replace('doi.org1','doi.org/1')\n",
    "                        if 'http://' in str(identifier.url) and not 'doi.org' in str(identifier.url):\n",
    "                            identifier.url=str(identifier.url).replace('http://', 'https://doi.org/')\n",
    "                        elif 'https://' in str(identifier.url) and not 'doi.org' in str(identifier.url):\n",
    "                            identifier.url=str(identifier.url).replace('https://', 'https://doi.org/')\n",
    "                        else:\n",
    "                            identifier.url=str(identifier.url)\n",
    "                        if not entry.doi or entry.doi==\"\":\n",
    "                            entry.doi=identifier.url\n",
    "                        elif identifier.url != entry.doi and identifier.url not in entry.duplicate_ids['doi']:\n",
    "                            entry.duplicate_ids['doi'].append(identifier.url)\n",
    "                        matched=True\n",
    "            if not matched:\n",
    "                if identifier.idtype==\"other\":\n",
    "                    entry.other_links['other'].append(identifier.url)\n",
    "                else:\n",
    "                    print(\"idtype not found/not matched\", identifier.idtype, identifier.url)\n",
    "    bulklist.append(entry)\n",
    "    if len(bulklist)==1000:\n",
    "        with transaction.atomic():\n",
    "            PureEntry.objects.bulk_update(bulklist, ['doi', 'isbn', 'researchutwente', 'risutwente', 'scopus', 'other_links', 'duplicate_ids'])\n",
    "        bulklist=[]\n",
    "        i=i+1000\n",
    "        print(str(i) + \" entries done\")\n",
    "        print(str(j) + \" identifiers processed in total\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PureOpenAlex.data_repair import matchAFASwithAuthor\n",
    "results=matchAFASwithAuthor()\n",
    "\n",
    "space=\"\"\n",
    "accepted=[]\n",
    "rejected=[]\n",
    "for result in results:\n",
    "    if result[1]==1.0:\n",
    "        accepted.append(result)\n",
    "        continue\n",
    "    curlen=len(f\"{result[2].first} {result[2].last}\")\n",
    "    if curlen > len(space):\n",
    "        space=\" \".join([\"\" for x in range(curlen)])\n",
    "    rejected.append(result)\n",
    "\n",
    "i=0\n",
    "keep=[2,9,11,13,19]\n",
    "for result in rejected:\n",
    "    acceptedcheck=\"\"\n",
    "    extraspace=\"\"\n",
    "    extranum=5\n",
    "    if i<10:\n",
    "        extraspace= \" \"\n",
    "    if i in keep:\n",
    "        accepted.append(result)\n",
    "        acceptedcheck=\"[X]\"\n",
    "        extranum=2\n",
    "\n",
    "    curspace=\" \".join([\"\" for x in range(extranum+len(space)-len(f\"{result[2].first} {result[2].last}\"))])\n",
    "\n",
    "\n",
    "    print(f\"[{i}]{acceptedcheck} {result[2].first} {result[2].last}{curspace}{extraspace}[{int(result[1]*100)}]   {result[3]}\")\n",
    "    i+=1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PureOpenAlex.models import Author\n",
    "space=\"\"\n",
    "for result in accepted:\n",
    "    curlen=len(f\"{result[2].first} {result[2].last}\")\n",
    "    if curlen > len(space):\n",
    "        space=\" \".join([\"\" for x in range(curlen)])\n",
    "i=0\n",
    "accept=[]\n",
    "other=[]\n",
    "reject=[]\n",
    "\n",
    "#dict: first one is the i-index of result, second is 0 (no match), 1 (first match), 2 (second match), 3 (other)\n",
    "# if there is only 1 match alway accept expect if overruled by the dict below\n",
    "# if there are more than 2 matches mark as other.\n",
    "\n",
    "final={5:1, 6:2, 8:3, 15:2, 17:3, 18:3, 21:3, 26:3, 28:3, 29:3, 30:2, 31:2, 34:0, 38:1, 43:3, 47:3, 48:3, 50:0, 57:1, 59:0, 62:3, 63:3, 65:1, 67:0, 68:3, }\n",
    "\n",
    "for result in accepted:\n",
    "    print(\"---------------------\")\n",
    "    matchedauthors=Author.objects.filter(name__icontains=\" \".join([result[2].first, result[2].last]))\n",
    "    if matchedauthors.count()==0:\n",
    "        matchedauthors=Author.objects.filter(first_name__icontains=result[2].first, last_name__icontains=result[2].last)\n",
    "        if matchedauthors.count()==0:\n",
    "            matchedauthors=Author.objects.filter(last_name__icontains=result[2].last)\n",
    "\n",
    "\n",
    "    curspace=\" \".join([\"\" for x in range(2+len(space)-len(f\"{result[2].first} {result[2].last}\"))])\n",
    "    extraspace=\"\"\n",
    "    if result[1]!=1.0:\n",
    "        extraspace=\" \"\n",
    "\n",
    "    print(f\"[{i}] {result[2].first} {result[2].last}{curspace}[{int(result[1]*100)}]{extraspace}   {result[3]}\")\n",
    "    if matchedauthors.count()==2:\n",
    "        print(f\"          2 matches found: {matchedauthors.first().name} and {matchedauthors.last().name}\")\n",
    "        if final[i]==0:\n",
    "            print(\"Discarded.\")\n",
    "            reject.append([result,None])\n",
    "        elif final[i]==1:\n",
    "            print(f\"Accepted {matchedauthors.first().name}.\")\n",
    "            accept.append([result,matchedauthors.first()])\n",
    "        else:\n",
    "            print(f\"Accepted {matchedauthors.last().name}.\")\n",
    "            accept.append([result,matchedauthors.last()])\n",
    "    elif matchedauthors.count()>1:\n",
    "        print(f\"          {matchedauthors.count()} matches found.\")\n",
    "        print(\"To others.\")\n",
    "        other.append([result,matchedauthors])\n",
    "    elif matchedauthors.count()==0:\n",
    "        print(f\"          No matches found.\")\n",
    "        reject.append([result,None])\n",
    "    else:\n",
    "        print(f\"          Match: {matchedauthors.first().name}\")\n",
    "        try:\n",
    "            if final[i]==0:\n",
    "                print(\"!DISCARDED!\")\n",
    "                reject.append([result,None])\n",
    "            elif final[i]==1:\n",
    "                print(f\"Accepted.\")\n",
    "                accept.append([result,matchedauthors.first()])\n",
    "            elif final[i]==3:\n",
    "                print(f'To others.')\n",
    "                other.append([result,matchedauthors.first()])\n",
    "        except:\n",
    "            print(\"Accepted.\")\n",
    "            accept.append([result,matchedauthors.first()])\n",
    "    i+=1\n",
    "\n",
    "print(f\"Accepted: {len(accept)}, Rejected: {len(reject)}, Other: {len(other)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "matching={0:42, 2:2, 3:28, 4:1, 7:0, 12:10, 15:10, 16:11}\n",
    "extraaccepted=[]\n",
    "print(len(accept))\n",
    "print(len(reject))\n",
    "for entry in other:\n",
    "    #print(\"========================\")\n",
    "\n",
    "    #print(f\"[{i}]Name:\", entry[0][2])\n",
    "    #print(\"Found authors:\")\n",
    "    j=0\n",
    "    authorindex=None\n",
    "    try:\n",
    "        authorindex=matching[i]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if type(entry[1]) is not Author:\n",
    "        for author in entry[1]:\n",
    "            #print(f\"{i}:{j}\",author.name)\n",
    "            if authorindex is not None:\n",
    "                if j==authorindex:\n",
    "                    accept.append([entry,author])\n",
    "                    extraaccepted.append([entry,author])\n",
    "            j=j+1\n",
    "    else:\n",
    "        #print(f\"{i}:{j}\",author.name)\n",
    "        pass\n",
    "    i=i+1\n",
    "\n",
    "print(len(accept))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for entry in accept:\n",
    "    print(f\"accept[{i}] has a list with details (accept[{i}][0]) for author {entry[1].name} (accept[{i}][1]) \")\n",
    "    print(f\"[{i}][0][0]: openalex api response for author\")\n",
    "    print(f\"[{i}][0][1]: matching score\")\n",
    "    print(f\"[{i}][0][2]: initial matching name from AFASdata\")\n",
    "    print(f\"[{i}][0][3]: matched name in openalex\")\n",
    "    print(entry[0][2],\" -- \", entry[0][3])\n",
    "    i=i+1\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PureOpenAlex.models import AFASData\n",
    "from django.db import transaction\n",
    "for entry in accept:\n",
    "    try:\n",
    "        name=entry[0][2].full_name\n",
    "    except:\n",
    "        name = entry[0][0][2].full_name\n",
    "    afas=AFASData.objects.filter(name=name).first()\n",
    "    if afas:\n",
    "        with transaction.atomic():\n",
    "            entry[1].afas_data=afas\n",
    "            entry[1].save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PureOpenAlex.models import UTData, Department\n",
    "from django.db.models import Q, Count, Window, F, Min, Max\n",
    "from django.db.models.functions import RowNumber\n",
    "\n",
    "duplicates = (\n",
    "    UTData.objects.values(\"employee_id\")\n",
    "    .annotate(count=Count(\"employee_id\"))\n",
    "    .filter(count__gt=1)\n",
    ")\n",
    "for duplicate in duplicates:\n",
    "    responses_to_check = UTData.objects.filter(\n",
    "        employee_id=duplicate[\"employee_id\"]\n",
    "    ).annotate(\n",
    "        row_number=Window(\n",
    "            expression=RowNumber(),\n",
    "            partition_by=[F(\"employee_id\")],\n",
    "            order_by=F(\"avatar\").asc(),\n",
    "        )\n",
    "    )\n",
    "    with transaction.atomic():\n",
    "        responses_to_check.filter(row_number__gt=1).delete()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pshell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
