{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This sets up the Django environment '''\n",
    "import os\n",
    "import django\n",
    "from django.db.models import Count, Q, Prefetch, Exists, OuterRef\n",
    "from rich import print\n",
    "\n",
    "PROJECTPATH = \"\"\n",
    "os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"mus.settings\")\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"  # https://docs.djangoproject.com/en/4.1/topics/async/#async-safety\n",
    "django.setup()\n",
    "\n",
    "from django.conf import settings\n",
    "from loguru import logger\n",
    "import asyncio\n",
    "from pymongo import MongoClient\n",
    "from PureOpenAlex.models import DBUpdate\n",
    "\n",
    "MONGOURL = getattr(settings, \"MONGOURL\")\n",
    "\n",
    "MONGODB = MongoClient(MONGOURL)\n",
    "db=MONGODB[\"mus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xclass_refactor.other_apis_import import OpenAIREAPI\n",
    "from xclass_refactor.mus_mongo_client import MusMongoClient\n",
    "import httpx\n",
    "import rich\n",
    "client = httpx.Client()\n",
    "oa = OpenAIREAPI(MusMongoClient())\n",
    "print(oa.token, oa.refreshurl)\n",
    "url = 'https://api.openaire.eu/search/researchProducts'\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {oa.get_token()}'\n",
    "}\n",
    "params = {'doi':'10.1080/10438599.2012.656527'}\n",
    "r=client.get(url, headers=headers, params=params)\n",
    "rich.inspect(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from rich.table import Table, Column\n",
    "from rich.console import Console\n",
    "db = MONGODB['metadata_unificiation_system']\n",
    "colls = [MONGODB['metadata_unificiation_system']['works_openalex'],MONGODB['mus']['api_responses_works_openalex']]\n",
    "yeardict = defaultdict(int)\n",
    "paperlist = defaultdict(list)\n",
    "paperdict = defaultdict(dict)\n",
    "cons = Console()\n",
    "\n",
    "for i, coll in enumerate(colls):\n",
    "\n",
    "    for work in coll.find():\n",
    "        yeardict[work['publication_year']] += 1\n",
    "        paperdict[work['publication_year']][work['id']] = True\n",
    "        paperlist[work['publication_year']].append(work['id'])\n",
    "    table = Table(show_header=True, header_style=\"bold magenta\", title=f'result {i+1}')\n",
    "    table.add_column(\"year\", style='cyan')\n",
    "    table.add_column(\"yeardict\", justify=\"left\", style=\"green\", no_wrap=True)\n",
    "    table.add_column(\"paperdict\", justify=\"left\", style=\"red\", no_wrap=True)\n",
    "    table.add_column(\"paperlist\", justify=\"left\", style=\"yellow\", no_wrap=True)\n",
    "\n",
    "    for year in [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]:\n",
    "        table.add_row(str(year), str(yeardict[year]), str(len(paperdict[year])), str(len(paperlist[year])))\n",
    "    cons.print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PureOpenAlex.models import Paper, Journal\n",
    "\n",
    "journal = Journal.objects.filter(id__in=[16930,4789])\n",
    "for journal in journal:\n",
    "    print(journal.name, journal.id)\n",
    "\n",
    "    papers = journal.papers.all()\n",
    "    print(papers.count())\n",
    "    for paper in papers:\n",
    "        print(paper.title, paper.id)\n",
    "        print([author.name for author in paper.authors.all() if author.is_ut])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PureOpenAlex.models import Paper\n",
    "Paper.objects.remove_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PureOpenAlex.data_export import export_paper_data\n",
    "{'new_tcs_papers.csv': {\n",
    "    'filters': [['TCS',''],['start_date','2016-01-01'],['end_date','2024-12-31']],\n",
    "\n",
    "}\n",
    "}\n",
    "\n",
    "requests = {\n",
    "    'new_ee_papers.csv': {\n",
    "        'filters': [['EE',''],['start_date','2016-01-01'],['end_date','2024-12-31']],\n",
    "    },\n",
    "\n",
    "}\n",
    "\n",
    "export_paper_data(requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "tussen 1-1-2016 en 31-12-2023\n",
    "\n",
    "open access status\n",
    "\n",
    "Van alle output:\n",
    "\n",
    "% of gold, green, ...., open access per year\n",
    "Number of open access data-sets, as published per year and cumulative\n",
    "Number of open access software/design repositories, as published per year and cumulative\n",
    "\n",
    "'''\n",
    "\n",
    "from PureOpenAlex.models import Paper, Author, UTData, PureEntry, PilotPureData\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PureOpenAlex.constants import EEGROUPSABBR, TCSGROUPSABBR\n",
    "import csv\n",
    "eepapers = Paper.objects.filter_by([['EE',''],['start_date','2016-01-01'],['end_date','2024-12-31']]).get_table_prefetches()\n",
    "tcspapers = Paper.objects.filter_by([['TCS',''],['start_date','2016-01-01'],['end_date','2024-12-31']]).get_table_prefetches()\n",
    "eepapers.count()\n",
    "tcspapers.count()\n",
    "datalist= []\n",
    "csvs=[]\n",
    "years= [2016,2017,2018,2019,2020,2021,2022,2023,2024]\n",
    "oa_types=['gold','green','hybrid','bronze', 'closed']\n",
    "mus_url = 'https://openalex.samuelmok.cc/'\n",
    "mus_api_url = 'https://openalex.samuelmok.cc/api/'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see how many papers hav pure entries & pure pilot data\n",
    "for group in [eepapers, tcspapers]:\n",
    "    papers = 0\n",
    "    pure_entries = 0\n",
    "    paper_dupe_pure_entries = 0\n",
    "    pilot_data = 0\n",
    "    for paper in group:\n",
    "        papers += 1\n",
    "        if paper.pure_entries.first():\n",
    "            if paper.pure_entries.count() > 1:\n",
    "                paper_dupe_pure_entries += 1\n",
    "                for pure_entry in paper.pure_entries.all():\n",
    "                    pure_entries += 1\n",
    "                    if pure_entry.pilot_pure_data:\n",
    "                        pilot_data += 1\n",
    "            else:\n",
    "                pure_entries += 1\n",
    "                if paper.pure_entries.first().pilot_pure_data:\n",
    "                    pilot_data += 1\n",
    "\n",
    "    print(f''''checked {papers} papers, {paper_dupe_pure_entries} with multiple pure entries \\n\n",
    "        found {pure_entries} pure entries in total, of which {pilot_data} have pilot data''')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupn = 0\n",
    "for group in [eepapers, tcspapers]:\n",
    "    groupn += 1\n",
    "    csvoutput = []\n",
    "\n",
    "    for paper in group:\n",
    "        paperauthors=paper.authors.filter(utdata__isnull=False)\n",
    "        if groupn == 1:\n",
    "            groups = paperauthors.get_ut_groups(EEGROUPSABBR)\n",
    "        else:\n",
    "            groups = paperauthors.get_ut_groups(TCSGROUPSABBR)\n",
    "        mapping = {\n",
    "            'title':paper.title,\n",
    "            'doi':paper.doi,\n",
    "            'year':paper.year,\n",
    "            'itemtype':paper.itemtype,\n",
    "            'isbn':paper.pure_entries.first().isbn if paper.pure_entries.first() else '',\n",
    "            'topics':' | '.join([topic.get('display_name') for topic in paper.topics]) if paper.topics else '',\n",
    "            'Authorinfo ->':'',\n",
    "            'ut_authors':' | '.join([author.name for author in paperauthors]) if paperauthors else '',\n",
    "            'ut_groups': ' | '.join(groups) if groups else '',\n",
    "            'Openaccessinfo ->':'',\n",
    "            'is_openaccess':paper.is_oa,\n",
    "            'openaccess_type':paper.openaccess,\n",
    "            'found_as_green':paper.is_in_pure,\n",
    "            'present_in_pure':paper.has_pure_oai_match,\n",
    "            'license':paper.license,\n",
    "            'URLs ->':'',\n",
    "            'primary_link':paper.primary_link,\n",
    "            'pdf_link_primary':paper.pdf_link_primary,\n",
    "            'openalex_url':paper.openalex_url,\n",
    "            'pure_page_link':paper.pure_entries.first().researchutwente if paper.pure_entries.first() else '',\n",
    "            'pure_file_link':paper.pure_entries.first().risutwente if paper.pure_entries.first() else '',\n",
    "            'scopus_link':paper.pure_entries.first().scopus if paper.pure_entries.first() else '',\n",
    "            'Journalinfo ->':'',\n",
    "            'journal':paper.journal.name if paper.journal else '',\n",
    "            'journal_issn':paper.journal.issn if paper.journal else '',\n",
    "            'journal_e_issn':paper.journal.e_issn if paper.journal else '',\n",
    "            'journal_publisher':paper.journal.publisher if paper.journal else '',\n",
    "            'volume':paper.volume,\n",
    "            'issue':paper.issue,\n",
    "            'pages':paper.pages,\n",
    "            'pagescount':paper.pagescount,\n",
    "            'MUS links ->':'',\n",
    "            'mus_paper_details':mus_url+'paper/'+str(paper.id),\n",
    "            'mus_api_url_paper':mus_api_url+'paper/'+str(paper.id),\n",
    "        }\n",
    "        pureentrylist=''\n",
    "        pilotpuredatalist=''\n",
    "\n",
    "        if paper.pure_entries.first():\n",
    "            for pure_entry in paper.pure_entries.all():\n",
    "                if pureentrylist != '':\n",
    "                    pureentrylist = ' | '.join([pureentrylist, mus_api_url+'pureentry/'+str(pure_entry.id)])\n",
    "                else:\n",
    "                    pureentrylist = mus_api_url+'pureentry/'+str(pure_entry.id)\n",
    "                if pure_entry.pilot_pure_data:\n",
    "                    if pilotpuredatalist != '':\n",
    "                        pilotpuredatalist = ' | '.join([pilotpuredatalist, mus_api_url+'pilotpure/'+str(pure_entry.pilot_pure_data.id)])\n",
    "                    else:\n",
    "                        pilotpuredatalist = mus_api_url+'pilotpure/'+str(pure_entry.pilot_pure_data.id)\n",
    "\n",
    "        mapping['mus_api_url_pure_entry']=pureentrylist\n",
    "        mapping['mus_api_url_pure_report_details']=pilotpuredatalist\n",
    "        csvoutput.append(mapping)\n",
    "        if len(csvoutput) % 300 == 0:\n",
    "            print(f'{len(csvoutput)}/{len(group)} papers processed')\n",
    "\n",
    "    csvs.append(csvoutput)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['title',\n",
    "        'doi',\n",
    "        'year',\n",
    "        'itemtype',\n",
    "        'isbn',\n",
    "        'topics',\n",
    "        'Authorinfo ->',\n",
    "        'ut_authors',\n",
    "        'ut_groups',\n",
    "        'Openaccessinfo ->',\n",
    "        'is_openaccess',\n",
    "        'openaccess_type',\n",
    "        'found_as_green',\n",
    "        'present_in_pure',\n",
    "        'license',\n",
    "        'URLs ->',\n",
    "        'primary_link',\n",
    "        'pdf_link_primary',\n",
    "        'openalex_url',\n",
    "        'pure_page_link',\n",
    "        'pure_file_link',\n",
    "        'scopus_link',\n",
    "        'Journalinfo ->',\n",
    "        'journal',\n",
    "        'journal_issn',\n",
    "        'journal_e_issn',\n",
    "        'journal_publisher',\n",
    "        'volume',\n",
    "        'issue',\n",
    "        'pages',\n",
    "        'pagescount',\n",
    "        'MUS links ->',\n",
    "        'mus_paper_details',\n",
    "        'mus_api_url_paper',\n",
    "        'mus_api_url_pure_entry',\n",
    "        'mus_api_url_pure_report_details'\n",
    "    ]\n",
    "for file in ['ee_data.csv', 'tcs_data.csv']:\n",
    "    myFile = open(file, 'w', newline='',encoding='utf-8')\n",
    "    writer = csv.DictWriter(myFile, fieldnames=keys)\n",
    "    writer.writeheader()\n",
    "    if file == 'ee_data.csv':\n",
    "        writer.writerows(csvs[0])\n",
    "    else:\n",
    "        writer.writerows(csvs[1])\n",
    "    myFile.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_compact.groupby(['group', 'openaccess_type']).count())\n",
    "df_dropped=df_compact.drop('group',axis=1).drop_duplicates()\n",
    "print(df_dropped.groupby(['openaccess_type', 'itemtype']).count())\n",
    "print(df_dropped.groupby(['year']).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PureOpenAlex.models import Author, Paper, Authorship\n",
    "authors_prefetch =Prefetch(\n",
    "    'authors',\n",
    "    queryset=Author.objects.all().prefetch_related('authorships','affiliations', 'affils'),\n",
    ")\n",
    "papers = Paper.objects.filter(year__gte=2019).prefetch_related(authors_prefetch)\n",
    "\n",
    "results= {\n",
    "    'checked':0,\n",
    "    'matched':0,\n",
    "\n",
    "}\n",
    "changed_authorships = 0\n",
    "changed_papers = 0\n",
    "print(papers.count())\n",
    "for paper in papers:\n",
    "    results['checked']+=1\n",
    "    match = False\n",
    "    for authorship in paper.authorships.all():\n",
    "        author = authorship.author\n",
    "        if author.is_ut:\n",
    "            for affl in author.affiliations.all():\n",
    "                if 'twente' in affl.organization.name.lower():\n",
    "                    if int(paper.year) in affl.years:\n",
    "                        match = True\n",
    "                        authorship.ut_author_year_match = True\n",
    "                        authorship.save()\n",
    "                        changed_authorships +=1\n",
    "    if match:\n",
    "        paper.has_any_ut_author_year_match = True\n",
    "        paper.save()\n",
    "        changed_papers +=1\n",
    "        results['matched']+=1\n",
    "    if results['checked'] % 1000 == 0:\n",
    "        print(changed_authorships, changed_papers, results)\n",
    "\n",
    "\n",
    "print(changed_authorships, changed_papers)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PureOpenAlex.models import PureEntry, PilotPureData\n",
    "from django.db import transaction\n",
    "\n",
    "alltcsitems = PilotPureData.objects.all().only('doi','pureid','title')\n",
    "pureentries = PureEntry.objects.filter(year__gte=2018).only('id','doi', 'title', 'risutwente')\n",
    "k=0\n",
    "j=0\n",
    "z=0\n",
    "for item in alltcsitems:\n",
    "\n",
    "    purematch = pureentries.filter(doi=item.doi)\n",
    "    if not purematch:\n",
    "        purematch = pureentries.filter(risutwente__contains=item.pureid)\n",
    "    if not purematch:\n",
    "        purematch = pureentries.filter(title__iexact=item.title)\n",
    "\n",
    "    if not purematch:\n",
    "        k=k+1\n",
    "    else:\n",
    "        j=j+1\n",
    "        print(item.title)\n",
    "        purematchitem = purematch.first()\n",
    "        print(purematchitem)\n",
    "        if purematch.count()>1:\n",
    "            z=z+1\n",
    "        with transaction.atomic():\n",
    "            print(purematch.first().pilot_pure_data)\n",
    "            purematchitem.pilot_pure_data = item\n",
    "            purematchitem.save()\n",
    "            print(purematch.first().pilot_pure_data)\n",
    "\n",
    "\n",
    "\n",
    "    if j%100==0:\n",
    "        print(f\"{k} entries not matched\")\n",
    "        print(f\"{j} entries matched\")\n",
    "        print(f'{z} multiple matches')\n",
    "\n",
    "print(f\"{k} entries not matched\")\n",
    "print(f\"{j} entries matched\")\n",
    "print(f'{z} multiple matches')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PureOpenAlex.models import Author, Affiliation, Organization\n",
    "from PureOpenAlex.data_repair import fixMissingAffils\n",
    "\n",
    "fixMissingAffils()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from rich import print\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "client=MongoClient('mongodb://smops:bazending@192.168.2.153:27017/')\n",
    "db=client['mus']\n",
    "datasets=[]\n",
    "i=0\n",
    "openalex_works=db['api_responses_works_openalex']\n",
    "crossref_info=db['api_responses_crossref']\n",
    "for document in openalex_works.find():\n",
    "    crossrefdoc=None\n",
    "    try:\n",
    "        doi=document['doi'].replace('https://doi.org/','')\n",
    "        crossrefdoc=crossref_info.find_one({'DOI':doi})\n",
    "    except Exception as e:\n",
    "        print('error: ',e)\n",
    "        doi=None\n",
    "    dataset={\n",
    "        'works_openalex':document,\n",
    "        'crossref':crossrefdoc,\n",
    "    }\n",
    "    dataset['works_openalex']['_id']=str(dataset['works_openalex']['_id'])\n",
    "    try:\n",
    "        dataset['crossref']['_id']=str(dataset['crossref']['_id'])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    datasets.append(dataset)\n",
    "    with open(f'output_{i}.json', 'w') as f:\n",
    "        json.dump(dataset,f)\n",
    "    i=i+1\n",
    "    if i == 5:\n",
    "        break\n",
    "print(datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PureOpenAlex.models import PureAuthor, Author\n",
    "from django.db import transaction\n",
    "from nameparser import HumanName\n",
    "from PureOpenAlex.namematcher import NameMatcher\n",
    "from unidecode import unidecode\n",
    "from pprint import pprint\n",
    "\n",
    "allpureauthors = PureAuthor.objects.all()\n",
    "purenames={}\n",
    "purefullnames = {}\n",
    "pureinitials = {}\n",
    "for author in allpureauthors:\n",
    "    hname=HumanName(unidecode(author.name),initials_format=\"{first} {middle}\")\n",
    "    purenames[author.id] = {\n",
    "        'full': hname.full_name,\n",
    "        'initials': hname.initials()+\" \"+hname.last\n",
    "    }\n",
    "    purefullnames[hname.full_name]=author.id\n",
    "    pureinitials[hname.initials()+\" \"+hname.last]=author.id\n",
    "allauthors = Author.objects.all()\n",
    "\n",
    "authnames={}\n",
    "authfullnames = {}\n",
    "authinitials = {}\n",
    "for author in allauthors:\n",
    "    hname=HumanName(unidecode(author.name),initials_format=\"{first} {middle}\")\n",
    "    authnames[author.id] = {\n",
    "        'full': hname.full_name,\n",
    "        'initials': hname.initials()+\" \"+hname.last\n",
    "    }\n",
    "    authfullnames[hname.full_name]=author.id\n",
    "    authinitials[hname.initials()+\" \"+hname.last]=author.id\n",
    "\n",
    "\n",
    "purefullnameset=set(purefullnames.keys())\n",
    "pureinitialsset=set(pureinitials.keys())\n",
    "\n",
    "authfullnameset=set(authfullnames.keys())\n",
    "authinitialsset=set(authinitials.keys())\n",
    "\n",
    "print('# pure authors in db',allpureauthors.count())\n",
    "print('# pure authors in set (unique names)',len(purefullnameset))\n",
    "print('# authors in db',allauthors.count())\n",
    "print('# authors in set (unique names)',len(authfullnameset))\n",
    "\n",
    "intersection = purefullnameset.intersection(authfullnameset)\n",
    "print('# common names',len(intersection))\n",
    "\n",
    "listtosave=[]\n",
    "from PureOpenAlex.models import PureEntry\n",
    "\n",
    "j=0\n",
    "h=0\n",
    "\n",
    "for i,name in enumerate(intersection):\n",
    "    pureauthorid=purefullnames[name]\n",
    "    authorid=authfullnames[name]\n",
    "    pureauthor = PureAuthor.objects.get(id=pureauthorid)\n",
    "    pureentries = pureauthor.pure_entries.all()\n",
    "    pureentry_c = pureauthor.pure_creators.all()\n",
    "    author=Author.objects.get(id=authorid)\n",
    "    for entry in pureentries:\n",
    "        if author not in entry.authors.all():\n",
    "            entry.authors.add(author)\n",
    "            listtosave.append(entry)\n",
    "            h=h+1\n",
    "    for entry in pureentry_c:\n",
    "        if author not in entry.authors.all():\n",
    "            entry.authors.add(author)\n",
    "            listtosave.append(entry)\n",
    "            j=j+1\n",
    "    if i%1000==0:\n",
    "        print('# of entries that need updating:', len(listtosave))\n",
    "        print('# of intersections checked:', i)\n",
    "        print('pureentries:',h)\n",
    "        print('purecreators:',j)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PureOpenAlex.models import Author,PureEntry\n",
    "from django.db.models import Q\n",
    "noauths=PureEntry.objects.filter(authors__isnull=True).distinct()\n",
    "print(noauths.count())\n",
    "print(noauths.filter(Q(creators__isnull=False) | Q(contributors__isnull=False)).distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PureOpenAlex.models import Identifier, PureEntry\n",
    "from collections import defaultdict\n",
    "from django.db import transaction\n",
    "\n",
    "MATCHURLCONTENT = {\n",
    "    \"itc.utwente.nl\": \"itc_content\",\n",
    "    \"www.itc.nl\": \"itc_content\",\n",
    "    \"arxiv\": \"arxiv\",\n",
    "    \"zenodo\": \"zenodo\",\n",
    "    \"github\": \"github\",\n",
    "    \"https://10.\": \"doi\",\n",
    "    \"http://10.\": \"doi\",\n",
    "}\n",
    "MATCHIDTYPES = {\n",
    "\"doi\": '',\n",
    "\"isbn\": '',\n",
    "\"researchutwente\": '',\n",
    "\"risutwente\": '',\n",
    "\"scopus\": ''\n",
    "}\n",
    "bulklist=[]\n",
    "i=0\n",
    "j=0\n",
    "allentries=PureEntry.objects.all().filter(identifiers__isnull=False).only('doi', 'isbn', 'researchutwente', 'risutwente', 'scopus', 'other_links','id', 'duplicate_ids').prefetch_related(\"identifiers\")\n",
    "for entry in allentries:\n",
    "    entry.doi = \"\"\n",
    "    entry.isbn = \"\"\n",
    "    entry.researchutwente = \"\"\n",
    "    entry.risutwente = \"\"\n",
    "    entry.scopus = \"\"\n",
    "    entry.other_links= defaultdict(list)\n",
    "    entry.duplicate_ids = defaultdict(list)\n",
    "    for identifier in entry.identifiers.all():\n",
    "        j=j+1\n",
    "        duplicate=False\n",
    "        if 'https://ezproxy2.utwente.nl/login?url=' in identifier.url:\n",
    "            identifier.url = identifier.url.replace('https://ezproxy2.utwente.nl/login?url=','')\n",
    "        if str(identifier.idtype) in MATCHIDTYPES.keys():\n",
    "            if str(identifier.idtype) == 'doi':\n",
    "                identifier.url = identifier.url.replace('doi.org1','doi.org/1')\n",
    "                if entry.doi == \"\" or entry.doi == None:\n",
    "                    entry.doi = identifier.url\n",
    "                else:\n",
    "                    duplicate=True\n",
    "            if str(identifier.idtype) == 'isbn':\n",
    "                identifier.url = identifier.url.strip('urn:ISBN:')\n",
    "                if entry.isbn == \"\" or entry.isbn == None:\n",
    "                    entry.isbn = identifier.url\n",
    "                else:\n",
    "                    duplicate=True\n",
    "            if identifier.idtype == 'researchutwente':\n",
    "                if entry.researchutwente == \"\" or entry.researchutwente == None:\n",
    "                    entry.researchutwente = identifier.url\n",
    "                else:\n",
    "                    duplicate=True\n",
    "            if identifier.idtype == 'risutwente':\n",
    "                if entry.risutwente == \"\" or entry.risutwente == None:\n",
    "                    entry.risutwente = identifier.url\n",
    "                else:\n",
    "                    duplicate=True\n",
    "            if identifier.idtype == 'scopus':\n",
    "                if entry.scopus == \"\" or entry.scopus == None:\n",
    "                    entry.scopus = identifier.url\n",
    "                else:\n",
    "                    duplicate=True\n",
    "            if duplicate:\n",
    "                entry.duplicate_ids[str(identifier.idtype)].append(identifier.url)\n",
    "        else:\n",
    "            matched=False\n",
    "            for key, value in MATCHURLCONTENT.items():\n",
    "                if key in identifier.url and not matched:\n",
    "                    if value != \"doi\":\n",
    "                        entry.other_links[value].append(identifier.url)\n",
    "                        matched=True\n",
    "                    else: # doi with wrong formatting found\n",
    "                        identifier.url = identifier.url.replace('doi.org1','doi.org/1')\n",
    "                        if 'http://' in str(identifier.url) and not 'doi.org' in str(identifier.url):\n",
    "                            identifier.url=str(identifier.url).replace('http://', 'https://doi.org/')\n",
    "                        elif 'https://' in str(identifier.url) and not 'doi.org' in str(identifier.url):\n",
    "                            identifier.url=str(identifier.url).replace('https://', 'https://doi.org/')\n",
    "                        else:\n",
    "                            identifier.url=str(identifier.url)\n",
    "                        if not entry.doi or entry.doi==\"\":\n",
    "                            entry.doi=identifier.url\n",
    "                        elif identifier.url != entry.doi and identifier.url not in entry.duplicate_ids['doi']:\n",
    "                            entry.duplicate_ids['doi'].append(identifier.url)\n",
    "                        matched=True\n",
    "            if not matched:\n",
    "                if identifier.idtype==\"other\":\n",
    "                    entry.other_links['other'].append(identifier.url)\n",
    "                else:\n",
    "                    print(\"idtype not found/not matched\", identifier.idtype, identifier.url)\n",
    "    bulklist.append(entry)\n",
    "    if len(bulklist)==1000:\n",
    "        with transaction.atomic():\n",
    "            PureEntry.objects.bulk_update(bulklist, ['doi', 'isbn', 'researchutwente', 'risutwente', 'scopus', 'other_links', 'duplicate_ids'])\n",
    "        bulklist=[]\n",
    "        i=i+1000\n",
    "        print(str(i) + \" entries done\")\n",
    "        print(str(j) + \" identifiers processed in total\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PureOpenAlex.data_repair import matchAFASwithAuthor\n",
    "results=matchAFASwithAuthor()\n",
    "\n",
    "space=\"\"\n",
    "accepted=[]\n",
    "rejected=[]\n",
    "for result in results:\n",
    "    if result[1]==1.0:\n",
    "        accepted.append(result)\n",
    "        continue\n",
    "    curlen=len(f\"{result[2].first} {result[2].last}\")\n",
    "    if curlen > len(space):\n",
    "        space=\" \".join([\"\" for x in range(curlen)])\n",
    "    rejected.append(result)\n",
    "\n",
    "i=0\n",
    "keep=[2,9,11,13,19]\n",
    "for result in rejected:\n",
    "    acceptedcheck=\"\"\n",
    "    extraspace=\"\"\n",
    "    extranum=5\n",
    "    if i<10:\n",
    "        extraspace= \" \"\n",
    "    if i in keep:\n",
    "        accepted.append(result)\n",
    "        acceptedcheck=\"[X]\"\n",
    "        extranum=2\n",
    "\n",
    "    curspace=\" \".join([\"\" for x in range(extranum+len(space)-len(f\"{result[2].first} {result[2].last}\"))])\n",
    "\n",
    "\n",
    "    print(f\"[{i}]{acceptedcheck} {result[2].first} {result[2].last}{curspace}{extraspace}[{int(result[1]*100)}]   {result[3]}\")\n",
    "    i+=1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PureOpenAlex.models import Author\n",
    "space=\"\"\n",
    "for result in accepted:\n",
    "    curlen=len(f\"{result[2].first} {result[2].last}\")\n",
    "    if curlen > len(space):\n",
    "        space=\" \".join([\"\" for x in range(curlen)])\n",
    "i=0\n",
    "accept=[]\n",
    "other=[]\n",
    "reject=[]\n",
    "\n",
    "#dict: first one is the i-index of result, second is 0 (no match), 1 (first match), 2 (second match), 3 (other)\n",
    "# if there is only 1 match alway accept expect if overruled by the dict below\n",
    "# if there are more than 2 matches mark as other.\n",
    "\n",
    "final={5:1, 6:2, 8:3, 15:2, 17:3, 18:3, 21:3, 26:3, 28:3, 29:3, 30:2, 31:2, 34:0, 38:1, 43:3, 47:3, 48:3, 50:0, 57:1, 59:0, 62:3, 63:3, 65:1, 67:0, 68:3, }\n",
    "\n",
    "for result in accepted:\n",
    "    print(\"---------------------\")\n",
    "    matchedauthors=Author.objects.filter(name__icontains=\" \".join([result[2].first, result[2].last]))\n",
    "    if matchedauthors.count()==0:\n",
    "        matchedauthors=Author.objects.filter(first_name__icontains=result[2].first, last_name__icontains=result[2].last)\n",
    "        if matchedauthors.count()==0:\n",
    "            matchedauthors=Author.objects.filter(last_name__icontains=result[2].last)\n",
    "\n",
    "\n",
    "    curspace=\" \".join([\"\" for x in range(2+len(space)-len(f\"{result[2].first} {result[2].last}\"))])\n",
    "    extraspace=\"\"\n",
    "    if result[1]!=1.0:\n",
    "        extraspace=\" \"\n",
    "\n",
    "    print(f\"[{i}] {result[2].first} {result[2].last}{curspace}[{int(result[1]*100)}]{extraspace}   {result[3]}\")\n",
    "    if matchedauthors.count()==2:\n",
    "        print(f\"          2 matches found: {matchedauthors.first().name} and {matchedauthors.last().name}\")\n",
    "        if final[i]==0:\n",
    "            print(\"Discarded.\")\n",
    "            reject.append([result,None])\n",
    "        elif final[i]==1:\n",
    "            print(f\"Accepted {matchedauthors.first().name}.\")\n",
    "            accept.append([result,matchedauthors.first()])\n",
    "        else:\n",
    "            print(f\"Accepted {matchedauthors.last().name}.\")\n",
    "            accept.append([result,matchedauthors.last()])\n",
    "    elif matchedauthors.count()>1:\n",
    "        print(f\"          {matchedauthors.count()} matches found.\")\n",
    "        print(\"To others.\")\n",
    "        other.append([result,matchedauthors])\n",
    "    elif matchedauthors.count()==0:\n",
    "        print(f\"          No matches found.\")\n",
    "        reject.append([result,None])\n",
    "    else:\n",
    "        print(f\"          Match: {matchedauthors.first().name}\")\n",
    "        try:\n",
    "            if final[i]==0:\n",
    "                print(\"!DISCARDED!\")\n",
    "                reject.append([result,None])\n",
    "            elif final[i]==1:\n",
    "                print(f\"Accepted.\")\n",
    "                accept.append([result,matchedauthors.first()])\n",
    "            elif final[i]==3:\n",
    "                print(f'To others.')\n",
    "                other.append([result,matchedauthors.first()])\n",
    "        except:\n",
    "            print(\"Accepted.\")\n",
    "            accept.append([result,matchedauthors.first()])\n",
    "    i+=1\n",
    "\n",
    "print(f\"Accepted: {len(accept)}, Rejected: {len(reject)}, Other: {len(other)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "matching={0:42, 2:2, 3:28, 4:1, 7:0, 12:10, 15:10, 16:11}\n",
    "extraaccepted=[]\n",
    "print(len(accept))\n",
    "print(len(reject))\n",
    "for entry in other:\n",
    "    #print(\"========================\")\n",
    "\n",
    "    #print(f\"[{i}]Name:\", entry[0][2])\n",
    "    #print(\"Found authors:\")\n",
    "    j=0\n",
    "    authorindex=None\n",
    "    try:\n",
    "        authorindex=matching[i]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if type(entry[1]) is not Author:\n",
    "        for author in entry[1]:\n",
    "            #print(f\"{i}:{j}\",author.name)\n",
    "            if authorindex is not None:\n",
    "                if j==authorindex:\n",
    "                    accept.append([entry,author])\n",
    "                    extraaccepted.append([entry,author])\n",
    "            j=j+1\n",
    "    else:\n",
    "        #print(f\"{i}:{j}\",author.name)\n",
    "        pass\n",
    "    i=i+1\n",
    "\n",
    "print(len(accept))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for entry in accept:\n",
    "    print(f\"accept[{i}] has a list with details (accept[{i}][0]) for author {entry[1].name} (accept[{i}][1]) \")\n",
    "    print(f\"[{i}][0][0]: openalex api response for author\")\n",
    "    print(f\"[{i}][0][1]: matching score\")\n",
    "    print(f\"[{i}][0][2]: initial matching name from AFASdata\")\n",
    "    print(f\"[{i}][0][3]: matched name in openalex\")\n",
    "    print(entry[0][2],\" -- \", entry[0][3])\n",
    "    i=i+1\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PureOpenAlex.models import AFASData\n",
    "from django.db import transaction\n",
    "for entry in accept:\n",
    "    try:\n",
    "        name=entry[0][2].full_name\n",
    "    except:\n",
    "        name = entry[0][0][2].full_name\n",
    "    afas=AFASData.objects.filter(name=name).first()\n",
    "    if afas:\n",
    "        with transaction.atomic():\n",
    "            entry[1].afas_data=afas\n",
    "            entry[1].save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PureOpenAlex.models import UTData, Department\n",
    "from django.db.models import Q, Count, Window, F, Min, Max\n",
    "from django.db.models.functions import RowNumber\n",
    "\n",
    "duplicates = (\n",
    "    UTData.objects.values(\"employee_id\")\n",
    "    .annotate(count=Count(\"employee_id\"))\n",
    "    .filter(count__gt=1)\n",
    ")\n",
    "for duplicate in duplicates:\n",
    "    responses_to_check = UTData.objects.filter(\n",
    "        employee_id=duplicate[\"employee_id\"]\n",
    "    ).annotate(\n",
    "        row_number=Window(\n",
    "            expression=RowNumber(),\n",
    "            partition_by=[F(\"employee_id\")],\n",
    "            order_by=F(\"avatar\").asc(),\n",
    "        )\n",
    "    )\n",
    "    with transaction.atomic():\n",
    "        responses_to_check.filter(row_number__gt=1).delete()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
