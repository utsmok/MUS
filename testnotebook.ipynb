{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This sets up the Django environment '''\n",
    "import os\n",
    "import django\n",
    "from django.db.models import Count, Q, Prefetch, Exists, OuterRef\n",
    "from collections import defaultdict\n",
    "\n",
    "PROJECTPATH = \"\"\n",
    "os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"mus.settings\")\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"  # https://docs.djangoproject.com/en/4.1/topics/async/#async-safety\n",
    "django.setup()\n",
    "\n",
    "from django.conf import settings\n",
    "from loguru import logger\n",
    "import asyncio\n",
    "from pymongo import MongoClient\n",
    "from PureOpenAlex.models import DBUpdate\n",
    "\n",
    "MONGOURL = getattr(settings, \"MONGOURL\")\n",
    "\n",
    "MONGODB = MongoClient(MONGOURL)\n",
    "db=MONGODB[\"metadata_unification_system\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e453d99a797449fbd24b4e22524614d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Overview of publications by UT authors. \\n\\n                            Use widgetâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from rich import print\n",
    "\n",
    "# PUBLICATIONS\n",
    "# Load the data\n",
    "df = pd.read_csv('dataframe.csv')\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "# Create result_dict\n",
    "result_dict = {}\n",
    "result_dict['All'] = df.iloc[:, :9]\n",
    "for col in df.columns[9:]:\n",
    "    result_dict[col] = df[df[col]].iloc[:, :9]\n",
    "\n",
    "# Create widgets\n",
    "text_item = widgets.Label('''Overview of publications by UT authors. \\n\n",
    "                            Use widgets below to filter by group, year, and type. \\n''')\n",
    "\n",
    "group_dropdown = widgets.Dropdown(\n",
    "    options=list(result_dict.keys()),\n",
    "    description='Group:'\n",
    ")\n",
    "\n",
    "year_slider = widgets.IntRangeSlider(\n",
    "    value=[min(df['year'].min() for df in result_dict.values()),\n",
    "           max(df['year'].max() for df in result_dict.values())],\n",
    "    min=min(df['year'].min() for df in result_dict.values()),\n",
    "    max=max(df['year'].max() for df in result_dict.values()),\n",
    "    step=1,\n",
    "    description='Year Range:'\n",
    ")\n",
    "\n",
    "type_dropdown = widgets.Dropdown(\n",
    "    options=['All'] + list(set(type for df in result_dict.values() for type in df['type'].unique())),\n",
    "    description='Type:'\n",
    ")\n",
    "\n",
    "# Create output widget\n",
    "output = widgets.Output()\n",
    "\n",
    "# Define update function\n",
    "def update_output(*args):\n",
    "    with output:\n",
    "        output.clear_output(wait=True)\n",
    "        \n",
    "        # Get selected dataframe\n",
    "        df = result_dict[group_dropdown.value]\n",
    "        \n",
    "        # Apply filters\n",
    "        df = df[(df['year'] >= year_slider.value[0]) & (df['year'] <= year_slider.value[1])]\n",
    "        if type_dropdown.value != 'All':\n",
    "            df = df[df['type'] == type_dropdown.value]\n",
    "        \n",
    "        # Calculate rankings\n",
    "        publisher_ranking: pd.Series= df['publisher'].value_counts().head(25)\n",
    "        #journal_ranking = df['journal'].value_counts().head(25)\n",
    "        journal_ranking = df.groupby(['journal', 'publisher']).size().sort_values(ascending=False).head(25).reset_index(name='count')\n",
    "\n",
    "        # Display results\n",
    "        print(f\"Group: {group_dropdown.value}\")\n",
    "        print(f\"Year range: {year_slider.value[0]}-{year_slider.value[1]}\")\n",
    "        print(f\"Type: {type_dropdown.value}\")\n",
    "        print(f\"Total publications: {len(df)}\")\n",
    "        print(\"\\nTop Publishers:\")\n",
    "        display(HTML(publisher_ranking.to_frame().to_html()))\n",
    "        print(\"\\nTop Journals:\")\n",
    "        display(HTML(journal_ranking.to_html(index=False)))\n",
    "        \n",
    "        # Display interactive table\n",
    "\n",
    "# Link widgets to update function\n",
    "group_dropdown.observe(update_output, names='value')\n",
    "year_slider.observe(update_output, names='value')\n",
    "type_dropdown.observe(update_output, names='value')\n",
    "\n",
    "# Display widgets and output\n",
    "display(widgets.VBox([text_item, group_dropdown, year_slider, type_dropdown, output]))\n",
    "\n",
    "# Initial update\n",
    "update_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mus_wizard import models, constants\n",
    "from mus_wizard.harvester import openalex, oai_pmh\n",
    "from mus_wizard.database import matching, mongo_client\n",
    "from motor.motor_asyncio import AsyncIOMotorClient, AsyncIOMotorDatabase, AsyncIOMotorCollection\n",
    "\n",
    "authorid = 'A5045181048'\n",
    "mongoclient = mongo_client.MusMongoClient()\n",
    "\n",
    "openalex_authors: AsyncIOMotorCollection = mongoclient.authors_openalex\n",
    "pure_authors: AsyncIOMotorCollection = mongoclient.openaire_cris_persons\n",
    "openalex_works: AsyncIOMotorCollection = mongoclient.works_openalex\n",
    "pure_works: AsyncIOMotorCollection = mongoclient.openaire_cris_publications\n",
    "\n",
    "author_details = openalex_authors.find_one({'id': authorid})\n",
    "openalex_works.find({'authorships.author.id': authorid})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "from rich.console import Console\n",
    "cons = Console()\n",
    "# Testing retrieval of cerif data from oai_pmh endpoint\n",
    "import httpx\n",
    "import xmltodict\n",
    "import time\n",
    "\n",
    "def get_person_affiliations(values:list) -> tuple[str, list[dict]]:\n",
    "    affiliations = []\n",
    "    if isinstance(values, list):\n",
    "        for v in values:\n",
    "            tmp = v.get('cerif:OrgUnit')\n",
    "            org = {\n",
    "                'internal_repository_id': tmp.get('@id'),\n",
    "                'name': tmp.get('cerif:Name').get('#text'),\n",
    "            }\n",
    "            affiliations.append(org)\n",
    "    elif isinstance(values, str):\n",
    "        return values\n",
    "    return 'affiliations', affiliations\n",
    "\n",
    "def get_org_identifiers(values:list) -> tuple[str, dict[list]]:\n",
    "    identifiers = defaultdict(list)\n",
    "    if isinstance(values, list):\n",
    "        for v in values:\n",
    "            identifiers[v.get('@type')].append(v.get('#text'))\n",
    "    return 'identifiers', identifiers\n",
    "\n",
    "def get_org_part_of(value:dict|list) -> tuple[str, dict]|tuple[str, list[dict]]:\n",
    "    try:\n",
    "        if isinstance(value, dict):\n",
    "            part_of = {\n",
    "                'internal_repository_id': value.get('cerif:OrgUnit').get('@id'),\n",
    "                'name': value.get('cerif:OrgUnit').get('cerif:Name').get('#text'),\n",
    "            }\n",
    "        elif isinstance(value, list):\n",
    "            part_of = []\n",
    "            for v in value:\n",
    "                part_of.append({\n",
    "                    'internal_repository_id': v.get('cerif:OrgUnit').get('@id'),\n",
    "                    'name': v.get('cerif:OrgUnit').get('cerif:Name').get('#text'),\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f'error parsing {value}: {e}')\n",
    "        part_of = None\n",
    "    return 'part_of', part_of\n",
    "\n",
    "cerif_item_mapping = {\n",
    "    'persons':'cerif:Person',\n",
    "    'orgs': 'cerif:OrgUnit',\n",
    "    'works': 'cerif:Publication',\n",
    "    'products': 'cerif:Product',\n",
    "    'patents': 'cerif:Patent',\n",
    "    'datasets': 'cerif:Product',\n",
    "    'projects': 'cerif:Project',\n",
    "    'funding':'cerif:Funding',\n",
    "}\n",
    "\n",
    "cerif_mapping = {\n",
    "    'persons':{\n",
    "        'internal_repository_id':'@id',\n",
    "        'cerif:PersonName':{'family_names':'cerif:FamilyNames', 'first_names':'cerif:FirstNames'},\n",
    "        'orcid':'cerif:ORCID',\n",
    "        'scopus_id':'cerif:ScopusAuthorID',\n",
    "        'scopus_affil_id':'cerif:ScopusAffiliationID',\n",
    "        'cerif:Affiliation': get_person_affiliations,\n",
    "        'researcher_id': 'cerif:ResearcherID',\n",
    "        'isni': 'cerif:ISNI',\n",
    "        'cris-id': 'cerif:CRIS-ID',\n",
    "        'uuid':'cerif:UUID',\n",
    "        'uri':'cerif:URI',\n",
    "        'url':'cerif:URL',\n",
    "    },\n",
    "    'orgs': {\n",
    "        'internal_repository_id':'@id',\n",
    "        'cerif:Identifier':get_org_identifiers,\n",
    "        'cerif:Type':{'type':'#text'},\n",
    "        'cerif:PartOf':get_org_part_of,\n",
    "        'cerif:Name':{'name':'#text'},\n",
    "        'acronym':'cerif:Acronym',\n",
    "    },\n",
    "    'works': {},\n",
    "    'products': {},\n",
    "    'patents': {},\n",
    "    'datasets': {},\n",
    "    'projects': {},\n",
    "    'funding': {},\n",
    "    'ec_funded_resources': {},\n",
    "}\n",
    "\n",
    "cerif_keylist = {\n",
    "    'persons':['@id', 'cerif:PersonName', 'cerif:Affiliation', 'cerif:PersonName', 'cerif:Affiliation', 'cerif:ORCID', 'cerif:ScopusAuthorID', 'cerif:ScopusAffiliationID', 'cerif:ResearcherID', 'cerif:ISNI', 'cerif:CRIS-ID', 'cerif:UUID', 'cerif:URI', 'cerif:URL'],\n",
    "    'orgs': ['cerif:Identifier', 'cerif:Type', 'cerif:PartOf', 'cerif:Name', 'cerif:Acronym'],\n",
    "    'works': [],\n",
    "    'products': [],\n",
    "    'patents': [],\n",
    "    'datasets': [],\n",
    "    'projects': [],\n",
    "    'funding': [],\n",
    "    'ec_funded_resources': [],\n",
    "}\n",
    "def check_keys(item, keylist) -> list[str]:\n",
    "    missing_keys = []\n",
    "    for k in item.keys():\n",
    "        if k.startswith('cerif:') and k not in keylist:\n",
    "            missing_keys.append(k)\n",
    "    return missing_keys\n",
    "\n",
    "def process_cerif(type:str, data:list[dict]) -> list[dict]:\n",
    "    # TODO: handle nested fieldnames\n",
    "    keys_missing = set()\n",
    "    results=[]\n",
    "    mapping = cerif_mapping[type]\n",
    "    keylist = cerif_keylist[type]\n",
    "    for i in data:\n",
    "        item = i['metadata'].get(cerif_item_mapping[type])\n",
    "        result = {}\n",
    "        for key, value in mapping.items():\n",
    "            if isinstance(value, str):\n",
    "                result[key]=item.get(value)\n",
    "            elif isinstance(value, dict):\n",
    "                temp = item.get(key)\n",
    "                for k, v in value.items():\n",
    "                    result[k] = temp.get(v)\n",
    "            elif callable(value):\n",
    "                item_result = item.get(key)\n",
    "                if item_result:\n",
    "                    keyname, fullvalue = value(item_result)\n",
    "                    result[keyname] = fullvalue\n",
    "        missing = check_keys(item, keylist)\n",
    "        if missing:\n",
    "            [keys_missing.add(m) for m in missing]\n",
    "        results.append(result)\n",
    "    return results, keys_missing\n",
    "\n",
    "def get_results(url:str) -> list[dict]:\n",
    "    def fetch_response(url):\n",
    "        try:\n",
    "            r = httpx.get(url)\n",
    "            parsed = xmltodict.parse(r.text)\n",
    "            return parsed['OAI-PMH']['ListRecords']\n",
    "        except Exception as e:\n",
    "            print(f'error fetching {url}: {e}')\n",
    "            return None\n",
    "    results = []\n",
    "    resume_url = url.split('&metadataPrefix')[0]\n",
    "    while True:\n",
    "        response = fetch_response(url)\n",
    "        if not response:\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "        items = response.get('record')\n",
    "        if not isinstance(items, list):\n",
    "            items = [items]\n",
    "        for result in items:\n",
    "            results.append(result)\n",
    "        if response.get('resumptionToken'):\n",
    "            print(f'{response.get('resumptionToken').get('@cursor')}/{response.get(\"resumptionToken\").get(\"@completeListSize\")}')\n",
    "            resumetoken = response.get('resumptionToken').get('#text')\n",
    "            url = f\"{resume_url}&resumptionToken={resumetoken}\"\n",
    "        else:\n",
    "            return results\n",
    "\n",
    "base_url = 'https://ris.utwente.nl/ws/oai?verb=' # Use env variable\n",
    "verbs = {\n",
    "    'itemsets':'ListSets',\n",
    "    'schemas':'ListMetadataFormats',\n",
    "    'records':'ListRecords',\n",
    "    'identify':'Identify',\n",
    "}\n",
    "\n",
    "all_itemsets = {'persons':'openaire_cris_persons', 'orgs':'openaire_cris_orgunits','works':'openaire_cris_publications','products':'openaire_cris_products', 'patents':'openaire_cris_patents', 'datasets':'datasets:all', 'projects':'openaire_cris_projects', 'funding':'openaire_cris_funding'}\n",
    "itemsets = {'orgs':'openaire_cris_orgunits'}\n",
    "scheme = 'oai_cerif_openaire'\n",
    "finalresults = {}\n",
    "\n",
    "for type, itemset in itemsets.items():\n",
    "    singleresult = {}\n",
    "    url = f'https://ris.utwente.nl/ws/oai?verb=ListRecords&metadataPrefix={scheme}&set={itemset}'\n",
    "    #singleresult['raw'] = get_results(url)\n",
    "    singleresult['processed'], singleresult['missing_keys'] = process_cerif(type, get_results(url))\n",
    "    finalresults[type]=singleresult\n",
    "    collectionname=f'openaire_cris_{type}'\n",
    "    collection = db[collectionname]\n",
    "    print(f'Inserting {len(singleresult[\"processed\"])} {type} records into {collectionname}. Possibly missing keys: {singleresult[\"missing_keys\"]}')\n",
    "    collection.insert_many(singleresult['processed'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ut_orgs = []\n",
    "collection = db['openaire_cris_orgs']\n",
    "\n",
    "for org in collection.find():\n",
    "    if org.get('part_of') or 'Twente' in org.get('name') or org.get('name').startswith('UT'):\n",
    "        all_ut_orgs.append(org)\n",
    "\n",
    "print(f'Found {len(all_ut_orgs)} UT organisations')\n",
    "print(all_ut_orgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mus_wizard.models import Author, Topic, Organization, Group, Affiliation, Work\n",
    "from collections import defaultdict, Counter\n",
    "from rich import print, box\n",
    "from rich.table import Table\n",
    "from rich.console import Console\n",
    "cons = Console(record=True)\n",
    "\n",
    "authors_by_faculty = defaultdict(list)\n",
    "author_counts = {}\n",
    "total = 0\n",
    "# get list of authors by faculty\n",
    "for faculty in Group.Faculties.values:\n",
    "    faculty_groups = Group.objects.filter(faculty=faculty)\n",
    "    faculty_affiliations = Affiliation.objects.filter(groups__in=faculty_groups)\n",
    "    faculty_authors = Author.objects.filter(affiliation_details__in=faculty_affiliations)\n",
    "    author_counts[faculty] = len(faculty_authors) # store the counts for printing\n",
    "    total += len(faculty_authors)\n",
    "    authors_by_faculty[faculty].extend(faculty_authors) # store full list of authors to get topics\n",
    "\n",
    "# store the most common 5 topics for each faculty\n",
    "# this does not need to be a seperate loop, but this is easier to read/understand\n",
    "topics_by_faculty = defaultdict(list)\n",
    "fields_per_faculty = defaultdict(list)\n",
    "top_works_per_field = defaultdict(str)\n",
    "top_works_per_topic = defaultdict(str)\n",
    "top_authors_per_topic = defaultdict(str)\n",
    "\n",
    "\n",
    "for faculty, authors in authors_by_faculty.items():\n",
    "    faculty_topics = Topic.objects.filter(authors__in=authors)\n",
    "    topics_by_faculty[faculty].extend(Counter([t for t in faculty_topics]).most_common(5))\n",
    "    fields_per_faculty[faculty].extend(Counter([t.field for t in faculty_topics]))\n",
    "\n",
    "\n",
    "for faculty, topics in topics_by_faculty.items():\n",
    "    for topic in topics:\n",
    "        works = Work.objects.filter(topics=topic[0]).order_by('-cited_by_count')\n",
    "        top_works_per_topic[topic[0].openalex_id] = \"\\n\".join([f'- {work.title} ({work.cited_by_count})' for work in works[:3]])\n",
    "        authors = Author.objects.filter(topics=topic[0]).order_by('-works_count')\n",
    "        top_authors_per_topic[topic[0].openalex_id] = \"\\n\".join([f'- [link={author.openalex_id}]{author.name}[/link] ({author.works_count})' for author in authors[:3]])\n",
    "\n",
    "\n",
    "\n",
    "result = Table(title='Authors per faculty', title_style='deep_pink2', show_header=True)\n",
    "result.add_column('faculty', style='cyan')\n",
    "result.add_column('# authors', style='orange1')\n",
    "for k,v in author_counts.items():\n",
    "    result.add_row(k, str(v))\n",
    "cons.print(result)\n",
    "\n",
    "for faculty, topics in topics_by_faculty.items():\n",
    "    if faculty == Group.Faculties.OTHER:\n",
    "        continue\n",
    "    result2 = Table(title=f'Top topics for [bold bright_magenta]{faculty}[/bold bright_magenta]', title_style='deep_pink2', show_header=True, show_lines=True)\n",
    "    result2.add_column('topics', style='orange1')\n",
    "    result2.add_column('linked to # authors', style='pale_violet_red1')\n",
    "    result2.add_column('top 3 works (#citations)', style='deep_pink2')\n",
    "    result2.add_column('top 3 authors (#works)', style='deep_pink2')\n",
    "    for topic in topics:\n",
    "        result2.add_row(topic[0].name, str(topic[1]), str(top_works_per_topic[topic[0].openalex_id]), str(top_authors_per_topic[topic[0].openalex_id]))\n",
    "\n",
    "    cons.print(result2)\n",
    "\n",
    "#from rich.terminal_theme import SVG_EXPORT_THEME\n",
    "\n",
    "#cons.save_svg(\"topics_faculty.svg\", title=\"Author & Topic count per faculty\", theme=SVG_EXPORT_THEME)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find & remove duplicate model instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PureOpenAlex.models import UTData, Department\n",
    "from django.db.models import Q, Count, Window, F, Min, Max\n",
    "from django.db.models.functions import RowNumber\n",
    "\n",
    "duplicates = (\n",
    "    UTData.objects.values(\"employee_id\")\n",
    "    .annotate(count=Count(\"employee_id\"))\n",
    "    .filter(count__gt=1)\n",
    ")\n",
    "for duplicate in duplicates:\n",
    "    responses_to_check = UTData.objects.filter(\n",
    "        employee_id=duplicate[\"employee_id\"]\n",
    "    ).annotate(\n",
    "        row_number=Window(\n",
    "            expression=RowNumber(),\n",
    "            partition_by=[F(\"employee_id\")],\n",
    "            order_by=F(\"avatar\").asc(),\n",
    "        )\n",
    "    )\n",
    "    with transaction.atomic():\n",
    "        responses_to_check.filter(row_number__gt=1).delete()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
